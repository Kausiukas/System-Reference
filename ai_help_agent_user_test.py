#!/usr/bin/env python3
"""
AI Help Agent - Real User Validation Test Interface
==================================================

This interactive test validates that the AI Help Agent can function as a senior developer
assistant with full system integration capabilities.

Test Categories:
1. System Status & Health Analysis
2. Code Analysis & Explanation  
3. Performance & Error Analysis
4. Development Recommendations
5. Goal Tracking & Suggestions

Usage: python ai_help_agent_user_test.py
"""

import asyncio
import os
import time
import json
from datetime import datetime, timezone
from typing import Dict, List, Any
import streamlit as st
import tempfile
import zipfile
import requests
import base64
from pathlib import Path
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
from background_agents.ai_help.ai_help_agent import AIHelpAgent, HelpRequest
from background_agents.coordination.shared_state import SharedState
import asyncio
import threading
import time
import json
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from typing import Dict, Any, List
import asyncpg
import os
from dotenv import load_dotenv
import streamlit as st

# Load environment variables
load_dotenv()

# Global thread pool executor for async operations
_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="StreamlitAsync")

# Simple HelpRequest class for processing
class HelpRequest:
    def __init__(self, request_id: str, user_id: str, query: str, context: dict = None, timestamp=None, priority: str = "normal"):
        self.request_id = request_id
        self.user_id = user_id
        self.query = query
        self.context = context or {}
        self.timestamp = timestamp or datetime.now()
        self.priority = priority

def safe_async_run(async_func, *args, **kwargs):
    """Safely run async functions in streamlit environment with proper concurrency control"""
    def run_in_thread():
        # Create completely isolated event loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            # Create a simple wrapper that doesn't use semaphores to avoid cross-loop issues
            result = loop.run_until_complete(async_func(*args, **kwargs))
            return result
        except Exception as e:
            print(f"AsyncIO Error: {e}")
            return {"success": False, "error": str(e)}
        finally:
            # Aggressive cleanup to prevent loop conflicts
            try:
                # Cancel all tasks
                pending = asyncio.all_tasks(loop)
                if pending:
                    for task in pending:
                        task.cancel()
                    # Don't wait for completion to avoid deadlocks
                    
            except Exception:
                pass
            finally:
                try:
                    loop.close()
                except Exception:
                    pass
    
    try:
        # Use thread pool but with shorter timeout to avoid hanging
        future = _executor.submit(run_in_thread)
        return future.result(timeout=10)  # Reduced timeout
    except Exception as e:
        print(f"Async execution error: {e}")
        return {"success": False, "error": str(e)}

class SafeDatabaseClient:
    """Read-only database client that doesn't interfere with running background agents"""
    
    def __init__(self):
        self._connection = None
        self._lock = threading.Lock()
    
    async def get_connection(self):
        """Get a single read-only connection"""
        if self._connection is None:
            # Database configuration from environment
            host = os.getenv('POSTGRESQL_HOST', 'localhost')
            port = int(os.getenv('POSTGRESQL_PORT', 5432))
            database = os.getenv('POSTGRESQL_DATABASE', 'background_agents')
            user = os.getenv('POSTGRESQL_USER', 'postgres')
            password = os.getenv('POSTGRESQL_PASSWORD', '')
            
            # Create a single connection (not a pool to avoid conflicts)
            self._connection = await asyncpg.connect(
                host=host,
                port=port,
                user=user,
                password=password,
                database=database,
                server_settings={
                    'application_name': 'streamlit_readonly_client',
                    'default_transaction_isolation': 'read committed'
                }
            )
            print("âœ… Created read-only database connection")
        
        return self._connection
    
    async def get_registered_agents(self):
        """Get registered agents using read-only query"""
        try:
            conn = await self.get_connection()
            
            # Simple read-only query to avoid conflicts
            agents = await conn.fetch("""
                SELECT agent_id, agent_type, state, capabilities, 
                       created_at, last_seen
                FROM agents 
                ORDER BY created_at DESC
            """)
            
            # Convert to dict format
            agent_list = []
            for agent in agents:
                agent_dict = {
                    'agent_id': agent['agent_id'],
                    'agent_type': agent['agent_type'],
                    'state': agent['state'],
                    'capabilities': agent['capabilities'],
                    'created_at': agent['created_at'],
                    'last_seen': agent['last_seen']
                }
                agent_list.append(agent_dict)
            
            return agent_list
            
        except Exception as e:
            print(f"Error getting agents: {e}")
            return []
    
    async def get_performance_metrics(self):
        """Get performance metrics using read-only query"""
        try:
            conn = await self.get_connection()
            
            # Simple read-only query
            metrics = await conn.fetch("""
                SELECT metric_name, metric_value, metric_unit, agent_id, timestamp
                FROM performance_metrics 
                WHERE timestamp > NOW() - INTERVAL '24 hours'
                ORDER BY timestamp DESC
                LIMIT 100
            """)
            
            # Convert to dict format
            metric_list = []
            for metric in metrics:
                metric_dict = {
                    'metric_name': metric['metric_name'],
                    'value': metric['metric_value'],
                    'unit': metric['metric_unit'], 
                    'agent_id': metric['agent_id'],
                    'timestamp': metric['timestamp']
                }
                metric_list.append(metric_dict)
            
            return metric_list
            
        except Exception as e:
            print(f"Error getting metrics: {e}")
            return []
    
    async def get_system_events(self):
        """Get system events using read-only query"""
        try:
            conn = await self.get_connection()
            
            # Simple read-only query
            events = await conn.fetch("""
                SELECT event_type, event_data, agent_id, severity, timestamp
                FROM system_events 
                WHERE timestamp > NOW() - INTERVAL '24 hours'
                ORDER BY timestamp DESC
                LIMIT 50
            """)
            
            # Convert to dict format
            event_list = []
            for event in events:
                event_dict = {
                    'event_type': event['event_type'],
                    'event_data': event['event_data'],
                    'agent_id': event['agent_id'],
                    'severity': event['severity'],
                    'timestamp': event['timestamp']
                }
                event_list.append(event_dict)
            
            return event_list
            
        except Exception as e:
            print(f"Error getting events: {e}")
            return []
    
    async def close(self):
        """Close the database connection"""
        if self._connection:
            try:
                await self._connection.close()
                self._connection = None
                print("âœ… Closed read-only database connection")
            except Exception as e:
                print(f"âš ï¸ Error closing connection: {e}")

class StreamlitConnectionManager:
    """Manages database connections for Streamlit with safe read-only access"""
    
    def __init__(self):
        self._clients = {}
        self._lock = threading.Lock()
    
    def get_connection_key(self):
        """Get unique connection key for current session"""
        if 'session_id' not in st.session_state:
            st.session_state.session_id = f"session_{int(time.time())}_{hash(threading.current_thread())}"
        return st.session_state.session_id
    
    async def get_database_client(self):
        """Get or create safe database client for current session"""
        session_key = self.get_connection_key()
        
        with self._lock:
            if session_key not in self._clients:
                # Create new read-only client for this session
                client = SafeDatabaseClient()
                self._clients[session_key] = client
                print(f"âœ… Created read-only database client for {session_key}")
            
            return self._clients[session_key]
    
    async def close_connection(self, session_key: str = None):
        """Close connection for specific session"""
        if session_key is None:
            session_key = self.get_connection_key()
            
        with self._lock:
            if session_key in self._clients:
                try:
                    await self._clients[session_key].close()
                    del self._clients[session_key]
                    print(f"âœ… Closed database client for {session_key}")
                except Exception as e:
                    print(f"âš ï¸ Error closing client: {e}")
    
    async def close_all_connections(self):
        """Close all connections"""
        with self._lock:
            session_keys = list(self._clients.keys())
            
        for session_key in session_keys:
            await self.close_connection(session_key)

# Global connection manager
connection_manager = StreamlitConnectionManager()

# User feedback logging
def log_user_feedback(question: str, response: str, user_satisfied: bool, user_comment: str, metadata: Dict):
    """Log user feedback to file for system improvement"""
    feedback_entry = {
        'timestamp': datetime.now().isoformat(),
        'question': question,
        'response': response[:500] + '...' if len(response) > 500 else response,
        'user_satisfied': user_satisfied,
        'user_comment': user_comment,
        'metadata': metadata,
        'session_id': st.session_state.get('session_id', 'unknown')
    }
    
    try:
        # Create feedback directory if it doesn't exist
        feedback_dir = Path('feedback_logs')
        feedback_dir.mkdir(exist_ok=True)
        
        # Log to daily file
        feedback_file = feedback_dir / f"user_feedback_{datetime.now().strftime('%Y%m%d')}.jsonl"
        
        with open(feedback_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(feedback_entry, ensure_ascii=False) + '\n')
            
        return True
    except Exception as e:
        st.error(f"Failed to log feedback: {e}")
        return False

class DatabaseConnectedAIAgent:
    """AI Help Agent that uses direct database access to provide real system insights"""
    
    def __init__(self, database_client):
        self.database_client = database_client
        self.agent_id = "database_connected_ai_help"
    
    async def process_single_request(self, request):
        """Process help request using real database data"""
        try:
            # Gather real system data
            agents_data = await self.database_client.get_registered_agents()
            metrics_data = await self.database_client.get_performance_metrics()
            events_data = await self.database_client.get_system_events()
            
            # Analyze the query and provide intelligent responses
            query = request.query.lower()
            
            # Generate contextual response based on query type and real data
            if any(word in query for word in ['status', 'health', 'agents', 'active']):
                response_text = self._generate_system_status_response(agents_data, metrics_data, events_data)
            elif any(word in query for word in ['performance', 'metrics', 'cpu', 'memory']):
                response_text = self._generate_performance_response(metrics_data, agents_data)
            elif any(word in query for word in ['events', 'errors', 'issues', 'problems']):
                response_text = self._generate_events_response(events_data, agents_data)
            elif any(word in query for word in ['database', 'postgresql', 'db']):
                response_text = self._generate_database_response(metrics_data, events_data)
            else:
                response_text = self._generate_general_response(query, agents_data, metrics_data, events_data)
            
            # Calculate confidence based on data availability
            confidence = self._calculate_confidence(agents_data, metrics_data, events_data)
            
            # Create response object
            class DatabaseResponse:
                def __init__(self, text, confidence, processing_time, business_value):
                    self.response_text = text
                    self.confidence_score = confidence
                    self.processing_time = processing_time
                    self.business_value = business_value
                    self.sources = ['Real-time Database', 'Agent Registry', 'Performance Metrics', 'System Events']
            
            return DatabaseResponse(
                text=response_text,
                confidence=confidence,
                processing_time=0.5,  # Fast database queries
                business_value=confidence * 0.8  # Business value tied to confidence
            )
            
        except Exception as e:
            # Error response
            class ErrorResponse:
                def __init__(self, error_msg):
                    self.response_text = f"âŒ **Database Query Error**: {error_msg}\n\nThe system encountered an issue while accessing the database. This may indicate:\n- Database connectivity problems\n- Query execution issues\n- Data inconsistencies"
                    self.confidence_score = 0
                    self.processing_time = 0.1
                    self.business_value = 0
                    self.sources = ['Error Handler']
            
            return ErrorResponse(str(e))
    
    def _generate_system_status_response(self, agents_data, metrics_data, events_data):
        """Generate system status response using real data"""
        active_agents = len([a for a in agents_data if a.get('state') in ['active', 'running']])
        total_agents = len(agents_data)
        
        # Calculate system health
        health_ratio = active_agents / max(total_agents, 1)
        if health_ratio >= 0.8:
            health_status = "ğŸŸ¢ Healthy"
        elif health_ratio >= 0.6:
            health_status = "ğŸŸ¡ Degraded"
        elif health_ratio >= 0.4:
            health_status = "ğŸŸ  Warning"
        else:
            health_status = "ğŸ”´ Critical"
        
        response = f"""## ğŸ–¥ï¸ Background Agents System Status

**ğŸ“Š System Overview:**
- **Agent Status**: {active_agents}/{total_agents} agents active
- **System Health**: {health_status} ({health_ratio*100:.1f}%)
- **Database Connection**: âœ… Active (read-only)
- **Recent Events**: {len(events_data)} events in last 24h
- **Performance Metrics**: {len(metrics_data)} data points collected

**ğŸ”§ Active Agents:**"""
        
        # List active agents
        for agent in agents_data:
            if agent.get('state') in ['active', 'running']:
                agent_type = agent.get('agent_type', 'Unknown')
                last_heartbeat = agent.get('last_heartbeat', 'Never')
                response += f"\n- **{agent['agent_id']}** ({agent_type}) - Last seen: {last_heartbeat}"
        
        # Add system recommendations
        if health_ratio < 0.8:
            response += f"\n\n**âš ï¸ Recommendations:**\n- {total_agents - active_agents} agents are inactive - check agent health\n- Review recent system events for issues\n- Consider restarting inactive agents"
        
        return response
    
    def _generate_performance_response(self, metrics_data, agents_data):
        """Generate performance analysis using real metrics data"""
        if not metrics_data:
            return "âš ï¸ **No Performance Data Available**\n\nNo performance metrics found in the last 24 hours. This could indicate:\n- Performance monitoring is disabled\n- Data collection issues\n- Database connectivity problems"
        
        # Analyze metrics by type
        cpu_metrics = [m for m in metrics_data if 'cpu' in m.get('metric_name', '').lower()]
        memory_metrics = [m for m in metrics_data if 'memory' in m.get('metric_name', '').lower()]
        
        response = f"""## ğŸ“ˆ System Performance Analysis

**ğŸ“Š Metrics Overview:**
- **Total Metrics**: {len(metrics_data)} data points (24h)
- **CPU Metrics**: {len(cpu_metrics)} measurements
- **Memory Metrics**: {len(memory_metrics)} measurements
- **Active Monitoring**: {len([a for a in agents_data if a.get('state') == 'active'])} agents

**ğŸ” Recent Performance Data:**"""
        
        # Show sample metrics
        for metric in metrics_data[:5]:
            metric_name = metric.get('metric_name', 'Unknown')
            value = metric.get('value', 0)
            unit = metric.get('unit', '')
            agent_id = metric.get('agent_id', 'System')
            response += f"\n- **{metric_name}**: {value}{unit} ({agent_id})"
        
        if len(metrics_data) > 5:
            response += f"\n- ... and {len(metrics_data) - 5} more metrics"
        
        return response
    
    def _generate_events_response(self, events_data, agents_data):
        """Generate events analysis using real events data"""
        if not events_data:
            return "âœ… **No Recent Events**\n\nNo system events recorded in the last 24 hours. This indicates:\n- System is running smoothly\n- No critical issues detected\n- Normal operation status"
        
        # Categorize events by severity
        critical_events = [e for e in events_data if e.get('severity') == 'CRITICAL']
        warning_events = [e for e in events_data if e.get('severity') == 'WARNING']
        info_events = [e for e in events_data if e.get('severity') == 'INFO']
        
        response = f"""## ğŸ“‹ System Events Analysis

**ğŸ“Š Events Summary (24h):**
- **Total Events**: {len(events_data)}
- **Critical**: {len(critical_events)} ğŸ”´
- **Warnings**: {len(warning_events)} ğŸŸ¡  
- **Information**: {len(info_events)} ğŸ”µ

**ğŸ” Recent Events:**"""
        
        # Show recent events
        for event in events_data[:5]:
            event_type = event.get('event_type', 'Unknown')
            severity = event.get('severity', 'INFO')
            timestamp = event.get('timestamp', 'Unknown')
            agent_id = event.get('agent_id', 'System')
            
            severity_icon = {'CRITICAL': 'ğŸ”´', 'WARNING': 'ğŸŸ¡', 'INFO': 'ğŸ”µ'}.get(severity, 'âšª')
            response += f"\n- {severity_icon} **{event_type}** ({agent_id}) - {timestamp}"
        
        if len(events_data) > 5:
            response += f"\n- ... and {len(events_data) - 5} more events"
        
        # Add recommendations for critical events
        if critical_events:
            response += f"\n\n**âš ï¸ Action Required:**\n- {len(critical_events)} critical events need attention\n- Review system logs for detailed error information\n- Consider immediate system health check"
        
        return response
    
    def _generate_database_response(self, metrics_data, events_data):
        """Generate database-specific response"""
        db_metrics = [m for m in metrics_data if any(term in m.get('metric_name', '').lower() for term in ['db', 'database', 'postgresql', 'query'])]
        db_events = [e for e in events_data if any(term in e.get('event_type', '').lower() for term in ['db', 'database', 'postgresql'])]
        
        response = f"""## ğŸ—„ï¸ PostgreSQL Database Status

**ğŸ“Š Database Metrics:**
- **Connection**: âœ… Active (read-only access)
- **Database Metrics**: {len(db_metrics)} measurements
- **Database Events**: {len(db_events)} recorded
- **Total Data Points**: {len(metrics_data)} metrics, {len(events_data)} events

**ğŸ” Database Performance:**"""
        
        if db_metrics:
            for metric in db_metrics[:3]:
                metric_name = metric.get('metric_name', 'Unknown')
                value = metric.get('value', 0)
                unit = metric.get('unit', '')
                response += f"\n- **{metric_name}**: {value}{unit}"
        else:
            response += "\n- No specific database metrics available"
            response += "\n- Database is accessible and responding"
            response += "\n- Query execution successful"
        
        return response
    
    def _generate_general_response(self, query, agents_data, metrics_data, events_data):
        """Generate general response for other queries"""
        return f"""## ğŸ¤– AI Help Assistant

**ğŸ” Query Processing**: Analyzed your question about "{query}"

**ğŸ“Š Current System Context:**
- **Agents**: {len([a for a in agents_data if a.get('state') == 'active'])}/{len(agents_data)} active
- **Metrics**: {len(metrics_data)} performance data points
- **Events**: {len(events_data)} system events
- **Database**: âœ… Connected and accessible

**â„¹ï¸ Response**: Based on the available system data, I can provide information about:
- System status and agent health
- Performance metrics and trends  
- System events and error analysis
- Database connectivity and status
- Code analysis and recommendations

**ğŸ’¡ Suggestion**: Please be more specific about what you'd like to know about the background agents system. I have access to real-time data and can provide detailed insights."""
    
    def _calculate_confidence(self, agents_data, metrics_data, events_data):
        """Calculate confidence based on data availability"""
        confidence = 0
        
        # Base confidence on data availability
        if agents_data:
            confidence += 30
        if metrics_data:
            confidence += 35
        if events_data:
            confidence += 25
        
        # Bonus for active agents
        if agents_data:
            active_ratio = len([a for a in agents_data if a.get('state') == 'active']) / max(len(agents_data), 1)
            confidence += active_ratio * 10
        
        return min(confidence, 95)  # Cap at 95% to be humble

class AIHelpAgentUserTest:
    def __init__(self):
        """Initialize the test agent"""
        self.ai_help_agent = None
        self.shared_state = None
        self.start_time = time.time()
        
        # Track user feedback instead of automatic success
        self.test_session = {
            'start_time': datetime.now(),
            'test_results': [],
            'user_feedback': [],  # New: track user satisfaction
            'total_questions': 0,
            'user_satisfied_count': 0,  # New: count user-validated successful responses
            'automatic_success_count': 0,  # Keep track of technical success vs user satisfaction
            'feedback_comments': []  # New: store user comments for analysis
        }
        
        # Generate unique session ID for feedback logging
        if 'session_id' not in st.session_state:
            st.session_state.session_id = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(str(datetime.now())) % 10000}"

    async def initialize(self):
        """Initialize with safe read-only database access"""
        try:
            # Use connection manager to get read-only database client
            self.database_client = await connection_manager.get_database_client()
            self.shared_state = None  # We're not using SharedState to avoid conflicts
            
            # Test connection with a quick agent access
            try:
                agents = await asyncio.wait_for(self.database_client.get_registered_agents(), timeout=5.0)
                print(f"âœ… Connected to background agents database: {len(agents)} agents found")
                
                # Count active agents
                active_agents = len([a for a in agents if a.get('state') in ['active', 'running']])
                print(f"   Active agents: {active_agents}/{len(agents)}")
                
            except Exception as e:
                print(f"âš ï¸ Database connection failed: {e}")
                return self._initialize_standalone_mode()
            
            # Create a database-connected AI Help Agent simulator
            # Since we can't use the real AIHelpAgent without SharedState conflicts,
            # we'll create a sophisticated mock that uses real database data
            self.ai_help_agent = DatabaseConnectedAIAgent(self.database_client)
            
            print("âœ… Database-connected AI Help Agent initialized successfully")
            self.connection_mode = "database_connected"
            return True
            
        except asyncio.TimeoutError:
            print("âŒ Database connection timeout")
            return self._initialize_standalone_mode()
        except Exception as e:
            print(f"âŒ Database initialization failed: {e}")
            return self._initialize_standalone_mode()
    
    def _initialize_standalone_mode(self):
        """Initialize in standalone mode when background agents aren't accessible"""
        try:
            print("ğŸ”„ Initializing in standalone mode...")
            
            # Create a minimal shared state for testing
            self.shared_state = None  # No shared state connection
            
            # Create a mock AI help agent for testing
            class MockAIHelpAgent:
                def __init__(self):
                    self.agent_id = "ai_help_test_standalone"
                    
                async def process_help_request(self, request):
                    return {
                        'success': True,
                        'response': f"âš ï¸ **Standalone Mode Response**: {request.query}\n\n"
                                  f"The AI Help Agent is running in standalone mode because it cannot connect "
                                  f"to the background agents system. This may be because:\n"
                                  f"- Background agents are not running\n"
                                  f"- PostgreSQL connection issues\n"
                                  f"- Network connectivity problems\n\n"
                                  f"To get real system data, please:\n"
                                  f"1. Start background agents: `python launch_background_agents.py`\n"
                                  f"2. Verify PostgreSQL is running\n"
                                  f"3. Restart this test interface",
                        'confidence_score': 95.0,
                        'processing_time': 0.1,
                        'sources': ['Standalone Test Mode'],
                        'business_value': 10.0
                    }
            
            self.ai_help_agent = MockAIHelpAgent()
            
            print("âœ… Standalone mode initialized - responses will indicate connection issues")
            return True
            
        except Exception as e:
            print(f"âŒ Even standalone initialization failed: {e}")
            return False

    async def process_user_question(self, question: str, category: str = "general", context: Dict = None) -> Dict[str, Any]:
        """Process user question with honest connection status reporting"""
        start_time = time.time()
        
        try:
            # Check if we have any data source available (database client or shared_state)
            has_data_source = (hasattr(self, 'database_client') and self.database_client) or (hasattr(self, 'shared_state') and self.shared_state)
            
            if not has_data_source:
                return self._create_standalone_response(question, category, start_time)
            
            # Test connection health BEFORE processing
            connection_status = await self._verify_connection_health()
            
            if not connection_status['healthy']:
                return self._create_connection_error_response(question, category, start_time, connection_status)
            
            # Try to gather real system data with individual error handling
            system_context = await self._gather_system_data_safely()
            
            # Verify we actually got meaningful data
            data_quality = self._assess_data_quality(system_context)
            
            if data_quality['score'] < 0.3:  # Less than 30% data availability
                return self._create_insufficient_data_response(question, category, start_time, system_context, data_quality)
            
            # Create help request only if we have good data
            help_request = HelpRequest(
                request_id=f"user_test_{int(time.time())}_{self.test_session['total_questions']}",
                user_id="test_user",
                query=question,
                context=context or {},
                timestamp=datetime.now(),
                priority="normal"
            )
            
            # Process with AI Help Agent
            try:
                response = await asyncio.wait_for(
                    self.ai_help_agent.process_single_request(help_request), timeout=15.0
                )
            except asyncio.TimeoutError:
                return self._create_timeout_response(question, category, start_time)
            
            # Validate response quality against actual data
            response_validation = self._validate_response_against_data(response, system_context)
            
            # Update session stats
            self.test_session['total_questions'] += 1
            confidence = response.confidence_score if hasattr(response, 'confidence_score') else 0
            
            # Only count as technical success if we have good data AND good response
            if confidence > 50 and data_quality['score'] > 0.7:
                self.test_session['automatic_success_count'] += 1
            
            # Build honest result
            result = {
                'success': data_quality['score'] > 0.7 and response_validation['valid'],
                'response': self._enhance_response_with_data_quality(response, data_quality, response_validation),
                'question': question,
                'category': category,
                'confidence': min(confidence, data_quality['score'] * 100),  # Cap confidence by data quality
                'quality_grade': self._calculate_honest_quality_grade(confidence, data_quality['score']),
                'processing_time': time.time() - start_time,
                'business_value': response.business_value if hasattr(response, 'business_value') else 0.0,
                'system_context': system_context,
                'sources': self._get_validated_sources(response, data_quality),
                'data_quality': data_quality,
                'response_validation': response_validation
            }
            
            self.test_session['test_results'].append(result)
            return result
            
        except Exception as e:
            return self._create_comprehensive_error_response(question, category, start_time, str(e))
    
    async def _verify_connection_health(self) -> Dict[str, Any]:
        """Verify database connection is actually working"""
        health_status = {
            'healthy': False,
            'can_get_agents': False,
            'can_get_metrics': False,
            'can_get_events': False,
            'error_details': []
        }
        
        # Check if we have database client (new mode) or shared_state (fallback)
        data_source = None
        if hasattr(self, 'database_client') and self.database_client:
            data_source = self.database_client
            print("âœ“ Using database client for connection testing")
        elif hasattr(self, 'shared_state') and self.shared_state:
            data_source = self.shared_state
            print("âœ“ Using shared_state for connection testing")
        else:
            health_status['error_details'].append("No data source available")
            print("âœ— No database client or shared_state available")
            return health_status
        
        try:
            # Test basic agent access
            agents = await asyncio.wait_for(data_source.get_registered_agents(), timeout=3.0)
            if agents and len(agents) > 0:
                health_status['can_get_agents'] = True
                print(f"âœ“ Connection test: Found {len(agents)} agents")
        except Exception as e:
            health_status['error_details'].append(f"Agent access failed: {e}")
            print(f"âœ— Connection test: Agent access failed - {e}")
        
        try:
            # Test metrics access
            metrics = await asyncio.wait_for(data_source.get_performance_metrics(), timeout=3.0)
            if metrics is not None:
                health_status['can_get_metrics'] = True
                print(f"âœ“ Connection test: Found {len(metrics)} metrics")
        except Exception as e:
            health_status['error_details'].append(f"Metrics access failed: {e}")
            print(f"âœ— Connection test: Metrics access failed - {e}")
        
        try:
            # Test events access
            events = await asyncio.wait_for(data_source.get_system_events(), timeout=3.0)
            if events is not None:
                health_status['can_get_events'] = True
                print(f"âœ“ Connection test: Found {len(events)} events")
        except Exception as e:
            health_status['error_details'].append(f"Events access failed: {e}")
            print(f"âœ— Connection test: Events access failed - {e}")
        
        # Connection is healthy if we can get at least agents data
        health_status['healthy'] = health_status['can_get_agents']
        
        return health_status
    
    async def _gather_system_data_safely(self) -> Dict[str, Any]:
        """Gather system data with individual error handling and honest reporting"""
        system_context = {
            'agents_active': 0,
            'system_health': 'unknown',
            'recent_events': 0,
            'query_category': 'general',
            'database_connected': False,
            'data_sources_available': [],
            'data_collection_errors': []
        }
        
        # Determine data source (database client or shared_state)
        data_source = None
        if hasattr(self, 'database_client') and self.database_client:
            data_source = self.database_client
            system_context['connection_mode'] = 'database_client'
        elif hasattr(self, 'shared_state') and self.shared_state:
            data_source = self.shared_state
            system_context['connection_mode'] = 'shared_state'
        else:
            system_context['data_collection_errors'].append("No data source available")
            return system_context
        
        # Try agents data
        try:
            agents_data = await asyncio.wait_for(data_source.get_registered_agents(), timeout=4.0)
            if agents_data:
                active_agents = len([a for a in agents_data if a.get('state') in ['active', 'running']])
                system_context['agents_active'] = active_agents
                system_context['total_agents'] = len(agents_data)
                system_context['database_connected'] = True
                system_context['data_sources_available'].append('agent_registry')
                print(f"âœ“ Real data: {active_agents}/{len(agents_data)} agents active")
        except Exception as e:
            system_context['data_collection_errors'].append(f"Agent data: {e}")
            print(f"âœ— Could not get agent data: {e}")
        
        # Try performance data
        try:
            performance_data = await asyncio.wait_for(data_source.get_performance_metrics(), timeout=4.0)
            if performance_data is not None:
                system_context['performance_metrics_count'] = len(performance_data)
                system_context['data_sources_available'].append('performance_metrics')
                # Calculate system health based on actual metrics
                if system_context['agents_active'] > 0:
                    health_ratio = system_context['agents_active'] / system_context.get('total_agents', 1)
                    if health_ratio >= 0.8:
                        system_context['system_health'] = 'healthy'
                    elif health_ratio >= 0.5:
                        system_context['system_health'] = 'degraded'
                    else:
                        system_context['system_health'] = 'critical'
                print(f"âœ“ Real data: {len(performance_data)} performance metrics")
        except Exception as e:
            system_context['data_collection_errors'].append(f"Performance data: {e}")
            print(f"âœ— Could not get performance data: {e}")
        
        # Try events data
        try:
            events_data = await asyncio.wait_for(data_source.get_system_events(), timeout=4.0)
            if events_data is not None:
                system_context['recent_events'] = len(events_data[:10])
                system_context['data_sources_available'].append('system_events')
                print(f"âœ“ Real data: {len(events_data)} system events")
        except Exception as e:
            system_context['data_collection_errors'].append(f"Events data: {e}")
            print(f"âœ— Could not get events data: {e}")
        
        return system_context
    
    def _assess_data_quality(self, system_context: Dict[str, Any]) -> Dict[str, Any]:
        """Assess the quality and completeness of gathered data"""
        available_sources = len(system_context.get('data_sources_available', []))
        total_possible_sources = 3  # agents, metrics, events
        errors = len(system_context.get('data_collection_errors', []))
        
        # Calculate data quality score
        quality_score = available_sources / total_possible_sources
        
        # Penalize for errors
        if errors > 0:
            quality_score *= max(0.3, 1 - (errors * 0.2))
        
        return {
            'score': quality_score,
            'available_sources': available_sources,
            'total_sources': total_possible_sources,
            'error_count': errors,
            'assessment': 'excellent' if quality_score >= 0.8 else
                         'good' if quality_score >= 0.6 else
                         'fair' if quality_score >= 0.4 else
                         'poor'
        }
    
    def _enhance_response_with_data_quality(self, response, data_quality: Dict, validation: Dict) -> str:
        """Enhance response with honest data quality information"""
        original_response = response.response_text if hasattr(response, 'response_text') else str(response)
        
        # Add data quality disclaimer
        quality_notice = f"\n\n**ğŸ” Data Quality Assessment**\n"
        quality_notice += f"- **Data Availability**: {data_quality['assessment'].title()} ({data_quality['score']*100:.1f}%)\n"
        quality_notice += f"- **Sources Available**: {data_quality['available_sources']}/{data_quality['total_sources']}\n"
        
        if data_quality['score'] < 0.7:
            quality_notice += f"- **âš ï¸ Warning**: Limited data access may affect response accuracy\n"
        
        if validation and not validation['valid']:
            quality_notice += f"- **âš ï¸ Response Validation**: {validation['issues']}\n"
        
        return original_response + quality_notice
    
    def _create_standalone_response(self, question: str, category: str, start_time: float) -> Dict[str, Any]:
        """Create a response for standalone mode."""
        response_text = f"**âš ï¸ System Connection Issue**\n\n"
        response_text += f"**Your Question**: {question}\n\n"
        response_text += f"**Issue**: Cannot connect to background agents system to provide real-time data.\n\n"
        response_text += f"**Possible Causes**:\n"
        response_text += f"- Background agents not running (try: `python launch_background_agents.py`)\n"
        response_text += f"- PostgreSQL connection issues\n"
        response_text += f"- Network connectivity problems\n\n"
        response_text += f"**Recommendation**: Start the background agents system and restart this test interface for full functionality."
        
        # Update session stats
        self.test_session['total_questions'] += 1
        
        result = {
            'success': False,
            'response': response_text,
            'question': question,
            'category': category,
            'confidence': 95.0,
            'quality_grade': 'D',
            'processing_time': time.time() - start_time,
            'business_value': 5.0,
            'system_context': {
                'agents_active': 0,
                'system_health': 'disconnected',
                'recent_events': 0,
                'query_category': category,
                'database_connected': False,
                'connection_error': 'Standalone mode'
            },
            'sources': ['Connection Diagnostic', 'Standalone Mode']
        }
        
        self.test_session['test_results'].append(result)
        return result
    
    def _create_connection_error_response(self, question: str, category: str, start_time: float, connection_status: Dict) -> Dict[str, Any]:
        """Create a response indicating a connection error."""
        error_message = "System connection issues detected. Cannot provide real-time data."
        if connection_status['error_details']:
            error_message += "\n\n**Details**: " + ", ".join(connection_status['error_details'])
        
        response_text = f"**âš ï¸ System Connection Issue**\n\n"
        response_text += f"**Your Question**: {question}\n\n"
        response_text += f"**Issue**: {error_message}\n\n"
        response_text += f"**Recommendation**: Please ensure the background agents system is running and accessible."
        
        # Update session stats
        self.test_session['total_questions'] += 1
        
        result = {
            'success': False,
            'response': response_text,
            'question': question,
            'category': category,
            'confidence': 0,
            'quality_grade': 'F',
            'processing_time': time.time() - start_time,
            'business_value': 0.0,
            'system_context': {
                'agents_active': 0,
                'system_health': 'error',
                'recent_events': 0,
                'query_category': category,
                'error_details': error_message
            },
            'sources': ['Connection Diagnostic']
        }
        
        self.test_session['test_results'].append(result)
        return result
    
    def _create_insufficient_data_response(self, question: str, category: str, start_time: float, system_context: Dict, data_quality: Dict) -> Dict[str, Any]:
        """Create a response indicating insufficient data."""
        response_text = f"**âš ï¸ Insufficient System Data**\n\n"
        response_text += f"**Your Question**: {question}\n\n"
        response_text += f"**Issue**: The AI Help Agent cannot provide a detailed response due to limited data access.\n\n"
        response_text += f"**Data Availability**: {data_quality['assessment'].title()} ({data_quality['score']*100:.1f}%)\n"
        response_text += f"**Sources Available**: {data_quality['available_sources']}/{data_quality['total_sources']}\n\n"
        
        if system_context.get('data_collection_errors'):
            response_text += f"**Errors Encountered**:\n"
            for error in system_context['data_collection_errors']:
                response_text += f"- {error}\n"
            response_text += "\n"
        
        response_text += f"**Recommendation**: Please ensure the background agents system is running and accessible to provide more data."
        
        # Update session stats
        self.test_session['total_questions'] += 1
        
        result = {
            'success': False,
            'response': response_text,
            'question': question,
            'category': category,
            'confidence': 0,
            'quality_grade': 'F',
            'processing_time': time.time() - start_time,
            'business_value': 0.0,
            'system_context': system_context,
            'sources': ['Data Collection Error']
        }
        
        self.test_session['test_results'].append(result)
        return result
    
    def _create_timeout_response(self, question: str, category: str, start_time: float) -> Dict[str, Any]:
        """Create a response indicating a timeout."""
        response_text = f"**â° Request Timeout**\n\n"
        response_text += f"**Your Question**: {question}\n\n"
        response_text += f"**Issue**: The system took too long to process your question. This may indicate system performance issues or high load.\n\n"
        response_text += f"**Recommendation**: Please try again or rephrase your question."
        
        # Update session stats
        self.test_session['total_questions'] += 1
        
        result = {
            'success': False,
            'response': response_text,
            'question': question,
            'category': category,
            'confidence': 0,
            'quality_grade': 'F',
            'processing_time': time.time() - start_time,
            'business_value': 0.0,
            'system_context': {
                'agents_active': 0,
                'system_health': 'unknown',
                'recent_events': 0,
                'query_category': category,
                'timeout_reason': 'System took too long'
            },
            'sources': ['Timeout Handler']
        }
        
        self.test_session['test_results'].append(result)
        return result
    
    def _validate_response_against_data(self, response, system_context: Dict) -> Dict[str, Any]:
        """Validate if the response is consistent with the gathered system context."""
        issues = []
        
        response_text = response.response_text if hasattr(response, 'response_text') else str(response)
        response_lower = response_text.lower()
        
        # Check agent-related claims
        if "agent" in response_lower or "active" in response_lower:
            actual_agents = system_context.get('agents_active', 0)
            if actual_agents == 0 and ("running normally" in response_lower or "agents active" in response_lower):
                issues.append(f"Response claims agents are active but actual count is {actual_agents}")
        
        # Check health claims
        if "healthy" in response_lower or "normal" in response_lower or "operational" in response_lower:
            actual_health = system_context.get('system_health', 'unknown')
            if actual_health in ['unknown', 'error', 'critical', 'degraded']:
                issues.append(f"Response claims system is healthy but actual status is '{actual_health}'")
        
        # Check performance claims
        if "performance" in response_lower and "good" in response_lower:
            if not system_context.get('performance_metrics_count', 0):
                issues.append("Response mentions good performance but no performance metrics available")
        
        # Check event claims
        if "recent" in response_lower and "event" in response_lower:
            actual_events = system_context.get('recent_events', 0)
            if actual_events == 0:
                issues.append(f"Response mentions recent events but actual count is {actual_events}")
        
        return {
            'valid': len(issues) == 0,
            'issues': '; '.join(issues) if issues else 'Response validated successfully'
        }
    
    def _get_validated_sources(self, response, data_quality: Dict) -> List[str]:
        """Get sources from response, ensuring they are valid based on data quality."""
        original_sources = response.sources if hasattr(response, 'sources') else ['AI Help Agent']
        data_sources = data_quality.get('available_sources', 0)
        
        if data_sources == 0:
            return ['No Real Data Sources', 'AI Knowledge Base Only']
        elif data_sources < 3:
            return ['Limited Data Sources'] + original_sources
        else:
            return original_sources
    
    def _calculate_honest_quality_grade(self, confidence: float, data_quality_score: float) -> str:
        """Calculate quality grade based on confidence and data quality."""
        # Cap the effective confidence by data quality
        effective_confidence = confidence * data_quality_score
        
        if effective_confidence >= 80:
            return 'A'
        elif effective_confidence >= 70:
            return 'B'
        elif effective_confidence >= 60:
            return 'C'
        elif effective_confidence >= 40:
            return 'D'
        else:
            return 'F'
    
    def _create_comprehensive_error_response(self, question: str, category: str, start_time: float, error_message: str) -> Dict[str, Any]:
        """Create a comprehensive error response."""
        # Update session stats
        self.test_session['total_questions'] += 1
        
        error_response = {
            'success': False,
            'error': f"Failed to process question: {error_message}",
            'question': question,
            'category': category,
            'confidence': 0,
            'quality_grade': 'F',
            'processing_time': time.time() - start_time,
            'business_value': 0.0,
            'system_context': {
                'agents_active': 0,
                'system_health': 'error',
                'recent_events': 0,
                'query_category': category,
                'error_details': error_message
            },
            'sources': ['Error Handler']
        }
        
        self.test_session['test_results'].append(error_response)
        return error_response

    def _calculate_quality_grade(self, confidence: float) -> str:
        """Calculate quality grade based on confidence score"""
        if confidence >= 90:
            return 'A'
        elif confidence >= 80:
            return 'B'
        elif confidence >= 70:
            return 'C'
        elif confidence >= 60:
            return 'D'
        else:
            return 'F'

    def get_session_stats(self) -> Dict[str, Any]:
        """Get current test session statistics"""
        duration = datetime.now() - self.test_session['start_time']
        
        # Calculate success rate based on user feedback
        user_success_rate = (self.test_session['user_satisfied_count'] / max(self.test_session['total_questions'], 1)) * 100
        
        # Calculate technical success rate for comparison
        technical_success_rate = (self.test_session['automatic_success_count'] / max(self.test_session['total_questions'], 1)) * 100
        
        # Calculate average confidence from all responses (both successful and failed)
        all_results = self.test_session['test_results']
        avg_confidence = sum(r.get('confidence', 0) for r in all_results) / max(len(all_results), 1)
        
        # Calculate average processing time
        avg_processing_time = sum(r.get('processing_time', 0) for r in self.test_session['test_results']) / max(len(self.test_session['test_results']), 1)
        
        return {
            'duration_minutes': duration.total_seconds() / 60,
            'questions_asked': self.test_session['total_questions'],
            'successful_responses': self.test_session['user_satisfied_count'],
            'success_rate': user_success_rate,
            'technical_success_rate': technical_success_rate,  # New: for comparison
            'avg_confidence': avg_confidence,
            'avg_processing_time': avg_processing_time,
            'pending_feedback': self.test_session['total_questions'] - len(self.test_session['user_feedback'])  # New: questions awaiting feedback
        }
    
    def record_user_feedback(self, question_index: int, user_satisfied: bool, user_comment: str = ""):
        """Record user feedback for a specific question"""
        if question_index < len(self.test_session['test_results']):
            # Update session tracking
            feedback_entry = {
                'question_index': question_index,
                'user_satisfied': user_satisfied,
                'user_comment': user_comment,
                'timestamp': datetime.now().isoformat()
            }
            
            self.test_session['user_feedback'].append(feedback_entry)
            self.test_session['feedback_comments'].append(user_comment)
            
            # Update satisfied count
            if user_satisfied:
                self.test_session['user_satisfied_count'] += 1
            
            # Log to file for system improvement
            result = self.test_session['test_results'][question_index]
            question = result.get('question', result.get('filename', 'Unknown'))
            response = result.get('response', result.get('analysis', 'No response'))
            
            log_user_feedback(
                question=question,
                response=response,
                user_satisfied=user_satisfied,
                user_comment=user_comment,
                metadata={
                    'confidence': result.get('confidence', 0),
                    'processing_time': result.get('processing_time', 0),
                    'category': result.get('category', 'general'),
                    'technical_success': result.get('success', False),
                    'system_health': result.get('system_context', {}).get('system_health', 'unknown')
                }
            )

    async def analyze_uploaded_code(self, code_content: str, filename: str = "uploaded_code.py", context: Dict = None) -> Dict[str, Any]:
        """Analyze uploaded code for integration assessment"""
        try:
            start_time = time.time()
            
            # Create a specialized help request for code analysis
            analysis_request = HelpRequest(
                request_id=f"code_analysis_{int(time.time())}",
                user_id="code_analyst",
                query=f"Analyze this code for integration into the background agents system: {filename}",
                context={
                    'code_content': code_content,
                    'filename': filename,
                    'analysis_type': 'integration_assessment',
                    'user_type': 'developer',
                    **(context or {})
                },
                timestamp=datetime.now(timezone.utc),
                priority='high',
                category='integration'
            )
            
            # Enhanced context with code analysis
            system_context = await self.ai_help_agent.context_integrator.gather_system_context(
                f"Code integration analysis for {filename}:\n```python\n{code_content[:1000]}...\n```"
            )
            system_context['code_analysis'] = {
                'filename': filename,
                'code_length': len(code_content),
                'code_preview': code_content[:500]
            }
            
            # Generate comprehensive analysis
            response = await self.ai_help_agent.rag_system.generate_response(
                f"Analyze this code for integration feasibility, system improvements, and provide integration steps:\n\nFile: {filename}\nCode:\n{code_content}",
                system_context
            )
            
            # Assess quality
            quality_assessment = await self.ai_help_agent.quality_assessor.assess_response_quality(analysis_request, response)
            
            processing_time = time.time() - start_time
            
            # Enhanced analysis result
            result = {
                'filename': filename,
                'code_length': len(code_content),
                'analysis': response.response_text,
                'confidence': response.confidence_score,
                'business_value': response.business_value,
                'sources': response.sources,
                'processing_time': processing_time,
                'quality_score': quality_assessment.get('overall_quality_score', 0),
                'quality_grade': quality_assessment.get('quality_grade', 'C'),
                'integration_assessment': {
                    'feasibility': 'high' if response.confidence_score > 80 else 'medium' if response.confidence_score > 60 else 'low',
                    'system_fit': response.confidence_score,
                    'improvement_potential': response.business_value
                },
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
            
            # Update test session statistics
            self.test_session['total_questions'] += 1
            if result['success']:
                self.test_session['user_satisfied_count'] += 1
            
            # Track in test results
            self.test_session['test_results'].append(result)
            
            return result
            
        except Exception as e:
            error_result = {
                'filename': filename,
                'error': str(e),
                'processing_time': time.time() - start_time,
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
            
            # Update test session statistics
            self.test_session['total_questions'] += 1
            
            # Track in test results
            self.test_session['test_results'].append(error_result)
            
            return error_result

    async def fetch_github_repo(self, repo_url: str) -> Dict[str, str]:
        """Fetch code files from GitHub repository"""
        try:
            # Extract owner and repo from URL
            if 'github.com' in repo_url:
                parts = repo_url.replace('https://github.com/', '').replace('http://github.com/', '').split('/')
                if len(parts) >= 2:
                    owner, repo = parts[0], parts[1]
                    
                    # GitHub API to get repository contents
                    api_url = f"https://api.github.com/repos/{owner}/{repo}/contents"
                    response = requests.get(api_url)
                    
                    if response.status_code == 200:
                        files = response.json()
                        code_files = {}
                        
                        for file_info in files:
                            if file_info['type'] == 'file' and file_info['name'].endswith(('.py', '.js', '.ts', '.java', '.cpp', '.c')):
                                # Get file content
                                file_response = requests.get(file_info['download_url'])
                                if file_response.status_code == 200:
                                    code_files[file_info['name']] = file_response.text
                                    
                                    # Limit to prevent overwhelming analysis
                                    if len(code_files) >= 5:
                                        break
                        
                        return code_files
                    else:
                        return {"error": f"Failed to access repository: {response.status_code}"}
                else:
                    return {"error": "Invalid GitHub URL format"}
            else:
                return {"error": "Please provide a valid GitHub URL"}
                
        except Exception as e:
            return {"error": f"Error fetching GitHub repository: {str(e)}"}

    async def cleanup(self):
        """Clean up resources using connection manager"""
        try:
            # Use connection manager to properly close connections
            await connection_manager.close_connection()
            print("âœ… Cleanup completed successfully")
        except Exception as e:
            # Log but don't fail on cleanup errors
            print(f"âš ï¸ Cleanup completed with minor issues: {e}")

# Streamlit UI
def main():
    st.set_page_config(
        page_title="AI Help Agent - Real User Test", 
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– AI Help Agent - Real User Validation Test")
    st.markdown("### Senior Developer Assistant Capabilities Test")
    
    # User Feedback System Information
    with st.expander("â„¹ï¸ About User Feedback Validation"):
        st.markdown("""
        **ğŸ”„ New Validation System**: This test now uses **user feedback** instead of automatic success measurement.
        
        **How it works:**
        - After each response, you'll be asked if it was helpful and accurate
        - Your feedback directly determines the system's success rate
        - Comments help improve future responses
        - All feedback is logged for system development
        
        **Why this matters:**
        - Technical success â‰  User satisfaction
        - Real user validation is more reliable
        - Your feedback helps identify real system issues
        - Production readiness based on actual user experience
        
        **ğŸ“Š Success Metrics:**
        - **User Success Rate**: Based on your satisfaction ratings
        - **Technical Success Rate**: Internal system metrics (for comparison)
        - **Gap Analysis**: Identifies when system thinks it's doing better than users feel
        """)
    
    st.divider()
    
    # Initialize session state
    if 'test_agent' not in st.session_state:
        st.session_state.test_agent = AIHelpAgentUserTest()
        st.session_state.initialized = False
    
    # Sidebar with test categories and predefined questions
    st.sidebar.header("ğŸ¯ Test Categories")
    
    test_categories = {
        "system_status": "System Status & Health",
        "code_analysis": "Code Analysis & Explanation", 
        "performance": "Performance & Error Analysis",
        "development": "Development Recommendations",
        "goals": "Goal Tracking & Suggestions",
        "integration": "New Agent Integration Guidance"
    }
    
    # Predefined test questions for each category
    predefined_questions = {
        "system_status": [
            "What is the current status of the background agents system?",
            "How many agents are currently active and what is the system health score?",
            "Are there any critical issues or errors I should be aware of?",
            "What is the current performance of the PostgreSQL database?",
            "Show me the recent system events and their impact."
        ],
        "code_analysis": [
            "Explain the architecture of the AI Help Agent implementation",
            "What does the SystemContextIntegrator class do and how does it work?",
            "Analyze the AdvancedRAGSystem and explain its key features",
            "Show me the most important code components in the background agents system",
            "What are the main classes in the agent coordination system?"
        ],
        "performance": [
            "What performance issues are currently affecting the system?",
            "Analyze the recent error logs and identify recurring problems",
            "What are the current memory and CPU usage patterns?",
            "Are there any performance bottlenecks I should address?",
            "What errors have occurred in the last 24 hours?"
        ],
        "development": [
            "What improvements should I make to the current system architecture?",
            "How can I optimize the performance of the background agents?",
            "What are the security considerations for this system?",
            "Suggest improvements for the database schema and queries",
            "What features should I prioritize for the next development sprint?"
        ],
        "goals": [
            "What are the current development goals and how are we progressing?",
            "Track the completion status of the TODO items in the system",
            "What are the highest priority tasks for system improvement?",
            "Suggest a roadmap for achieving production readiness",
            "What metrics should I track to measure system success?"
        ],
        "integration": [
            "How do I integrate a new monitoring agent into the background agents system?",
            "What are the steps to add a new data processing agent while maintaining system integrity?",
            "Guide me through creating a new business intelligence agent that follows existing patterns",
            "How should I implement a new security monitoring agent without disrupting current operations?",
            "What's the best way to add real-time notification capabilities to the system?",
            "How do I extend the PostgreSQL schema to support new agent types?",
            "Guide me through the process of adding new API endpoints for agent management",
            "Analyze uploaded code for integration feasibility and provide implementation steps"
        ]
    }
    
    # Initialization
    if not st.session_state.initialized:
        if st.button("ğŸš€ Initialize AI Help Agent", type="primary"):
            with st.spinner("Initializing AI Help Agent..."):
                success = safe_async_run(st.session_state.test_agent.initialize)
                
                if success:
                    st.session_state.initialized = True
                    st.success("âœ… AI Help Agent initialized successfully!")
                    st.rerun()
                else:
                    st.error("âŒ Failed to initialize AI Help Agent")
        return
    
    # Main interface
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ’¬ Ask the AI Help Agent")
        
        # Category selection
        selected_category = st.selectbox(
            "Select question category:",
            options=list(test_categories.keys()),
            format_func=lambda x: test_categories[x]
        )
        
        # Quick test buttons for predefined questions
        st.subheader("ğŸ¯ Quick Test Questions")
        for i, question in enumerate(predefined_questions[selected_category]):
            if st.button(f"ğŸ“ {question}", key=f"quick_{selected_category}_{i}"):
                st.session_state.current_question = question
                st.session_state.current_category = selected_category
        
        # Custom question input
        st.subheader("âœï¸ Custom Question")
        custom_question = st.text_area(
            "Enter your own question:",
            height=100,
            placeholder="Ask anything about system status, code analysis, performance, development recommendations, or goals..."
        )
        
        if st.button("ğŸ” Ask Custom Question", disabled=not custom_question):
            st.session_state.current_question = custom_question
            st.session_state.current_category = selected_category
        
        # Code Upload and Analysis Section (for integration category)
        if selected_category == "integration":
            st.subheader("ğŸ“ Code Upload & Analysis")
            st.markdown("**Upload code for integration assessment and guidance**")
            
            # Create tabs for different upload methods
            upload_tab1, upload_tab2 = st.tabs(["ğŸ“ File Upload", "ğŸ”— GitHub Repository"])
            
            with upload_tab1:
                st.markdown("**Upload Python files for analysis:**")
                uploaded_files = st.file_uploader(
                    "Choose Python files",
                    type=['py', 'txt'],
                    accept_multiple_files=True,
                    help="Upload Python files to analyze for integration into the background agents system"
                )
                
                if uploaded_files:
                    for uploaded_file in uploaded_files[:3]:  # Limit to 3 files
                        if st.button(f"ğŸ”¬ Analyze {uploaded_file.name}", key=f"analyze_{uploaded_file.name}"):
                            try:
                                code_content = str(uploaded_file.read(), "utf-8")
                                st.session_state.current_code_analysis = {
                                    'content': code_content,
                                    'filename': uploaded_file.name,
                                    'source': 'file_upload'
                                }
                                st.session_state.current_category = 'integration'
                            except Exception as e:
                                st.error(f"Error reading file: {e}")
            
            with upload_tab2:
                st.markdown("**Connect to GitHub repository:**")
                github_url = st.text_input(
                    "GitHub Repository URL",
                    placeholder="https://github.com/username/repository",
                    help="Enter a public GitHub repository URL to analyze its Python files"
                )
                
                if st.button("ğŸ”— Fetch & Analyze Repository", disabled=not github_url):
                    with st.spinner("Fetching repository files..."):
                        code_files = safe_async_run(
                            st.session_state.test_agent.fetch_github_repo, 
                            github_url
                        )
                        
                        if "error" in code_files:
                            st.error(code_files["error"])
                        elif code_files:
                            st.success(f"âœ… Found {len(code_files)} code files")
                            
                            # Let user select which file to analyze
                            selected_file = st.selectbox(
                                "Select file to analyze:",
                                options=list(code_files.keys()),
                                key="github_file_selector"
                            )
                            
                            if st.button(f"ğŸ”¬ Analyze {selected_file}", key=f"analyze_github_{selected_file}"):
                                st.session_state.current_code_analysis = {
                                    'content': code_files[selected_file],
                                    'filename': selected_file,
                                    'source': 'github',
                                    'repo_url': github_url
                                }
                                st.session_state.current_category = 'integration'
                        else:
                            st.warning("No Python files found in repository")
            
            # Sample code for testing
            with st.expander("ğŸ“ Sample Code for Testing"):
                sample_code = '''import asyncio
from datetime import datetime
from background_agents.coordination.base_agent import BaseAgent

class CustomMonitorAgent(BaseAgent):
    """Custom monitoring agent for system metrics"""
    
    def __init__(self, shared_state, config=None):
        super().__init__("custom_monitor_agent", shared_state, config)
        self.metrics_buffer = []
        
    async def initialize(self):
        """Initialize the monitoring agent"""
        await super().initialize()
        self.log_info("Custom Monitor Agent initialized")
        
    async def execute_work_cycle(self):
        """Execute monitoring work cycle"""
        try:
            # Collect system metrics
            metrics = await self.collect_system_metrics()
            
            # Process and store metrics
            await self.process_metrics(metrics)
            
            # Generate alerts if needed
            await self.check_alerts(metrics)
            
        except Exception as e:
            self.log_error(f"Work cycle failed: {e}")
            
    async def collect_system_metrics(self):
        """Collect various system metrics"""
        return {
            'timestamp': datetime.now(),
            'cpu_usage': 75.5,
            'memory_usage': 64.2,
            'active_connections': 12
        }
        
    async def process_metrics(self, metrics):
        """Process and store collected metrics"""
        await self.shared_state.log_performance_metric(
            self.agent_id,
            'system_health',
            metrics.get('cpu_usage', 0),
            metrics
        )
'''
                
                st.code(sample_code, language='python')
                if st.button("ğŸ”¬ Analyze Sample Code"):
                    st.session_state.current_code_analysis = {
                        'content': sample_code,
                        'filename': 'custom_monitor_agent.py',
                        'source': 'sample'
                    }
                    st.session_state.current_category = 'integration'
        
        # Process question or code analysis
        if hasattr(st.session_state, 'current_question') or hasattr(st.session_state, 'current_code_analysis'):
            
            if hasattr(st.session_state, 'current_code_analysis'):
                # Code analysis mode
                code_analysis = st.session_state.current_code_analysis
                st.subheader("ğŸ”¬ Code Integration Analysis")
                
                # Display code info
                col_info1, col_info2, col_info3 = st.columns(3)
                with col_info1:
                    st.metric("Filename", code_analysis['filename'])
                with col_info2:
                    st.metric("Source", code_analysis['source'].replace('_', ' ').title())
                with col_info3:
                    st.metric("Code Length", f"{len(code_analysis['content'])} chars")
                
                # Show code preview
                with st.expander("ğŸ“„ Code Preview"):
                    st.code(code_analysis['content'], language='python')
                
                with st.spinner(f"Analyzing {code_analysis['filename']} for integration..."):
                    result = safe_async_run(
                        st.session_state.test_agent.analyze_uploaded_code,
                        code_analysis['content'],
                        code_analysis['filename'],
                        {'source': code_analysis['source']}
                    )
                    
                # Clear code analysis
                delattr(st.session_state, 'current_code_analysis')
                
            else:
                # Regular question mode
                question = st.session_state.current_question
                category = st.session_state.current_category
                
                st.subheader("ğŸ¤– AI Help Agent Response")
                
                with st.spinner(f"Processing question: {question[:100]}..."):
                    result = safe_async_run(
                        st.session_state.test_agent.process_user_question,
                        question, 
                        category
                    )
                    
                # Clear current question
                delattr(st.session_state, 'current_question')
                delattr(st.session_state, 'current_category')
            
            if result.get('success'):
                if hasattr(st.session_state, 'current_code_analysis') or 'filename' in result:
                    # Code analysis result display
                    st.success("âœ… Code analysis completed successfully!")
                    
                    # Integration assessment
                    if 'integration_assessment' in result:
                        assessment = result['integration_assessment']
                        col_assess1, col_assess2, col_assess3 = st.columns(3)
                        
                        with col_assess1:
                            feasibility_color = {"high": "green", "medium": "orange", "low": "red"}.get(assessment['feasibility'], "gray")
                            st.markdown(f"**Integration Feasibility:** :{feasibility_color}[{assessment['feasibility'].upper()}]")
                        
                        with col_assess2:
                            st.metric("System Fit Score", f"{assessment['system_fit']:.1f}%")
                        
                        with col_assess3:
                            st.metric("Improvement Potential", f"${assessment['improvement_potential']:.2f}")
                    
                    # Analysis content
                    st.markdown("**Integration Analysis:**")
                    st.write(result['analysis'])
                    
                else:
                    # Regular response display
                    st.success("âœ… Response generated successfully!")
                    
                    # Response content
                    st.markdown("**Response:**")
                    st.write(result['response'])
                
                # Common metrics for both types
                col_a, col_b, col_c, col_d = st.columns(4)
                with col_a:
                    st.metric("Confidence", f"{result['confidence']:.1f}%")
                with col_b:
                    st.metric("Quality Grade", result['quality_grade'])
                with col_c:
                    st.metric("Processing Time", f"{result['processing_time']:.2f}s")
                with col_d:
                    st.metric("Business Value", f"${result['business_value']:.2f}")
                
                # System context information (for regular questions)
                if 'system_context' in result:
                    with st.expander("ğŸ“Š System Context Used"):
                        ctx = result['system_context']
                        st.write(f"**Active Agents:** {ctx['agents_active']}")
                        st.write(f"**System Health:** {ctx['system_health']}")
                        st.write(f"**Recent Events:** {ctx['recent_events']}")
                        st.write(f"**Query Category:** {ctx['query_category']}")
                        if 'sources' in result:
                            st.write(f"**Sources:** {', '.join(result['sources'])}")
                
                # Code analysis specific information
                if 'filename' in result:
                    with st.expander("ğŸ“ Code Analysis Details"):
                        st.write(f"**Filename:** {result['filename']}")
                        st.write(f"**Code Length:** {result['code_length']} characters")
                        if 'sources' in result:
                            st.write(f"**Analysis Sources:** {', '.join(result['sources'])}")
                
                # USER FEEDBACK VALIDATION - Replace automatic success measurement
                st.divider()
                st.subheader("ğŸ“ Response Validation")
                st.info("ğŸ” **Please evaluate the response quality** - Your feedback helps improve the system!")
                
                # Get the current result index
                current_result_index = len(st.session_state.test_agent.test_session['test_results']) - 1
                
                # Check if feedback already provided for this response
                existing_feedback = None
                for feedback in st.session_state.test_agent.test_session['user_feedback']:
                    if feedback['question_index'] == current_result_index:
                        existing_feedback = feedback
                        break
                
                if existing_feedback:
                    # Show existing feedback
                    st.success(f"âœ… Feedback recorded: {'Satisfied' if existing_feedback['user_satisfied'] else 'Not satisfied'}")
                    if existing_feedback['user_comment']:
                        st.write(f"**Comment:** {existing_feedback['user_comment']}")
                else:
                    # Collect new feedback
                    col_feedback1, col_feedback2 = st.columns([1, 2])
                    
                    with col_feedback1:
                        user_satisfied = st.radio(
                            "Was this response helpful and accurate?",
                            ["Select...", "âœ… Yes, satisfied", "âŒ No, not satisfied"],
                            key=f"satisfaction_{current_result_index}"
                        )
                    
                    with col_feedback2:
                        user_comment = st.text_area(
                            "Additional comments (optional):",
                            placeholder="What was good/bad? What could be improved? Any specific issues?",
                            key=f"comment_{current_result_index}",
                            height=100
                        )
                    
                    if user_satisfied != "Select...":
                        if st.button("ğŸ’¾ Submit Feedback", key=f"submit_{current_result_index}"):
                            satisfied = user_satisfied == "âœ… Yes, satisfied"
                            
                            # Record the feedback
                            st.session_state.test_agent.record_user_feedback(
                                current_result_index, 
                                satisfied, 
                                user_comment
                            )
                            
                            # Show confirmation
                            if satisfied:
                                st.success("âœ… Thank you! Response marked as satisfactory.")
                            else:
                                st.error("âŒ Thank you for the feedback. This will help improve the system.")
                            
                            st.rerun()  # Refresh to show updated stats
                    else:
                        st.warning("ğŸ‘† Please select whether you're satisfied with the response to continue.")
                
            else:
                error_message = result.get('error', 'Unknown error')
                st.error(f"âŒ Error processing request: {error_message}")
                
                # Even for errors, allow feedback to improve error handling
                st.divider()
                st.subheader("ğŸ“ Error Feedback")
                current_result_index = len(st.session_state.test_agent.test_session['test_results']) - 1
                
                error_comment = st.text_area(
                    "Help us improve error handling:",
                    placeholder="What went wrong? What did you expect to happen?",
                    key=f"error_comment_{current_result_index}"
                )
                
                if st.button("ğŸ’¾ Submit Error Feedback", key=f"error_submit_{current_result_index}"):
                    st.session_state.test_agent.record_user_feedback(
                        current_result_index,
                        False,  # Error responses are always unsatisfactory
                        f"ERROR: {error_message}. User comment: {error_comment}"
                    )
                    st.info("âœ… Error feedback recorded. Thank you!")
                    st.rerun()
    
    with col2:
        st.header("ğŸ“Š Test Session Stats")
        
        stats = st.session_state.test_agent.get_session_stats()
        
        # Main metrics
        col_stat1, col_stat2 = st.columns(2)
        with col_stat1:
            st.metric("Questions Asked", stats['questions_asked'])
            st.metric("User Success Rate", f"{stats['success_rate']:.1f}%")
            st.metric("Avg Response Time", f"{stats['avg_processing_time']:.2f}s")
        
        with col_stat2:
            st.metric("Pending Feedback", stats['pending_feedback'])
            st.metric("Technical Success", f"{stats['technical_success_rate']:.1f}%")
            st.metric("Session Duration", f"{stats['duration_minutes']:.1f} min")
        
        # Show feedback status
        pending_feedback = stats.get('pending_feedback', 0)
        if pending_feedback > 0:
            st.warning(f"â³ {pending_feedback} response(s) awaiting your feedback")
        elif stats['questions_asked'] > 0:
            st.success("âœ… All responses have been evaluated")
        
        # Show comparison if there's a gap between technical and user success
        if stats['questions_asked'] > 0:
            gap = stats.get('technical_success_rate', 0) - stats['success_rate']
            if gap > 10:
                st.error(f"âš ï¸ {gap:.1f}% gap between technical success and user satisfaction")
            elif gap > 0:
                st.warning(f"ğŸ“ˆ {gap:.1f}% difference - system thinks it's doing better than users feel")
            elif gap < -10:
                st.info(f"ğŸ‰ Users are {abs(gap):.1f}% more satisfied than technical metrics suggest!")
        
        # Test results history
        st.subheader("ğŸ“ Test History")
        for i, result in enumerate(reversed(st.session_state.test_agent.test_session['test_results'][-5:])):
            # Calculate actual index
            actual_index = len(st.session_state.test_agent.test_session['test_results']) - 1 - i
            
            # Check for user feedback on this result
            user_feedback = None
            for feedback in st.session_state.test_agent.test_session['user_feedback']:
                if feedback['question_index'] == actual_index:
                    user_feedback = feedback
                    break
            
            # Handle different result types
            if 'filename' in result:
                title = f"Code Analysis: {result['filename']}"
            else:
                title = f"Q{actual_index + 1}: {result.get('question', 'Unknown')[:30]}..."
            
            # Add feedback status to title
            if user_feedback:
                feedback_icon = "âœ…" if user_feedback['user_satisfied'] else "âŒ"
                title = f"{feedback_icon} {title}"
            else:
                title = f"â³ {title}"
            
            with st.expander(title):
                if result.get('success'):
                    if 'filename' in result:
                        # Code analysis result
                        st.write(f"**File:** {result['filename']}")
                        st.write(f"**Code Length:** {result.get('code_length', 0)} chars")
                        if 'integration_assessment' in result:
                            assessment = result['integration_assessment']
                            st.write(f"**Feasibility:** {assessment['feasibility'].upper()}")
                            st.write(f"**System Fit:** {assessment['system_fit']:.1f}%")
                        st.write(f"**Analysis:** {result.get('analysis', '')[:200]}...")
                    else:
                        # Regular question result
                        st.write(f"**Response:** {result.get('response', '')[:200]}...")
                    
                    st.write(f"**Confidence:** {result.get('confidence', 0):.1f}%")
                    st.write(f"**Quality:** {result.get('quality_grade', 'N/A')}")
                else:
                    st.error(f"Error: {result.get('error', 'Unknown')}")
                
                # Show user feedback if available
                if user_feedback:
                    st.divider()
                    feedback_color = "green" if user_feedback['user_satisfied'] else "red"
                    st.markdown(f"**User Feedback:** :{feedback_color}[{'Satisfied' if user_feedback['user_satisfied'] else 'Not Satisfied'}]")
                    if user_feedback['user_comment']:
                        st.write(f"**Comment:** {user_feedback['user_comment']}")
                else:
                    st.warning("â³ Awaiting user feedback")
        
        # Export results
        if st.button("ğŸ“ Export Test Results"):
            results_json = json.dumps(st.session_state.test_agent.test_session, indent=2, default=str)
            st.download_button(
                "ğŸ’¾ Download JSON",
                results_json,
                f"ai_help_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                "application/json"
            )
    
    # Final validation section
    st.header("ğŸ¯ Production Readiness Validation")
    
    # Check if we have enough feedback to assess readiness
    feedback_completion = (len(st.session_state.test_agent.test_session['user_feedback']) / max(stats['questions_asked'], 1)) * 100
    
    if stats['questions_asked'] >= 5 and stats.get('pending_feedback', 0) == 0:
        # Calculate production readiness score based on USER FEEDBACK
        readiness_score = 0
        
        # User satisfaction rate (50% weight) - Most important factor
        if stats['success_rate'] >= 90:
            readiness_score += 50
        elif stats['success_rate'] >= 80:
            readiness_score += 40
        elif stats['success_rate'] >= 70:
            readiness_score += 30
        elif stats['success_rate'] >= 60:
            readiness_score += 20
        
        # Performance (25% weight)
        if stats['avg_processing_time'] <= 2.0:
            readiness_score += 25
        elif stats['avg_processing_time'] <= 5.0:
            readiness_score += 20
        elif stats['avg_processing_time'] <= 10.0:
            readiness_score += 15
        
        # Coverage (15% weight) - check if multiple categories tested and code analysis included
        categories_tested = len(set(r.get('category', 'general') for r in st.session_state.test_agent.test_session['test_results']))
        code_analyses = len([r for r in st.session_state.test_agent.test_session['test_results'] if 'filename' in r])
        
        if categories_tested >= 5 and code_analyses >= 1:
            readiness_score += 15
        elif categories_tested >= 4:
            readiness_score += 12
        elif categories_tested >= 3:
            readiness_score += 9
        elif categories_tested >= 2:
            readiness_score += 6
        
        # System access capability (10% weight) - Check if system can access real data
        real_data_responses = len([r for r in st.session_state.test_agent.test_session['test_results'] 
                                 if r.get('system_context', {}).get('system_health', 'unknown') != 'unknown'])
        if real_data_responses > 0:
            readiness_score += 10
        elif stats['technical_success_rate'] > 50:
            readiness_score += 5  # Partial credit if technical success but no real data
        
        # Display readiness assessment
        col_x, col_y = st.columns(2)
        
        with col_x:
            st.metric("Production Readiness Score", f"{readiness_score}/100")
            
            if readiness_score >= 80:
                st.success("ğŸ‰ **PRODUCTION READY!** The AI Help Agent meets all criteria for production deployment.")
                st.balloons()
            elif readiness_score >= 60:
                st.warning("âš ï¸ **MOSTLY READY** - Some improvements needed before production.")
            else:
                st.error("âŒ **NOT READY** - Significant improvements required.")
        
        with col_y:
            # Detailed criteria based on user feedback
            st.subheader("ğŸ“‹ Readiness Criteria")
            criteria = [
                ("User Satisfaction â‰¥90%", stats['success_rate'] >= 90),
                ("Response Time â‰¤2s", stats['avg_processing_time'] <= 2.0),
                ("Multi-category Coverage", categories_tested >= 5),
                ("Code Analysis Capability", code_analyses >= 1),
                ("Real System Data Access", real_data_responses > 0),
                ("Min 6 Test Questions", stats['questions_asked'] >= 6),
                ("All Feedback Collected", stats.get('pending_feedback', 0) == 0)
            ]
            
            for criterion, met in criteria:
                st.write(f"{'âœ…' if met else 'âŒ'} {criterion}")
        
        # Show user feedback summary
        if st.session_state.test_agent.test_session['feedback_comments']:
            with st.expander("ğŸ“ User Feedback Summary"):
                for i, comment in enumerate(st.session_state.test_agent.test_session['feedback_comments']):
                    if comment.strip():
                        st.write(f"**Response {i+1}:** {comment}")
    
    elif stats.get('pending_feedback', 0) > 0:
        pending_count = stats.get('pending_feedback', 0)
        st.warning(f"â³ **Please provide feedback for {pending_count} response(s) to get production readiness assessment.**")
        st.info("ğŸ’¡ User feedback is required to accurately assess system readiness for production.")
    
    else:
        st.info("ğŸ’¡ **Test at least 6 questions across different categories and provide feedback to get a production readiness assessment.**")
    
    # Cleanup button
    if st.button("ğŸ§¹ End Test Session"):
        # Try cleanup but don't fail the session end if it has issues
        try:
            cleanup_result = safe_async_run(st.session_state.test_agent.cleanup)
            if cleanup_result and not cleanup_result.get('success', True):
                st.warning("âš ï¸ Session ended with minor cleanup issues (this is normal)")
        except Exception as e:
            st.warning(f"âš ï¸ Session ended with cleanup issues: {e}")
        
        # Clear session state regardless of cleanup success
        st.session_state.clear()
        st.success("âœ… Test session ended. Refresh page to start a new session.")
        st.rerun()

if __name__ == "__main__":
    main() 